<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
	<head>
		<title>Bayesiean Filtering Classes</title>
		<link media="all" href="/style.css" type="text/css" rel="Stylesheet">
	</head>
	<body vlink="blue" link="blue">
		<h1 align="center"><u>&nbsp;Bayesian Filtering Classes</u></h1>
		<div><a href="Class%20Documentation/html/index.html" target="_top"><span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana">Bayes++
Source Documentation</span></a><font face="Verdana"> Generated by <em>Doxygen</em></font>
		</div>
		<blockquote dir="ltr">
			<div><span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana"><a href="Class%20Documentation/html/hierarchy.html" target="_top"><span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana">Class
hierarchy of Bayes++</span></a></span></div>
			<span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana">
				<div><span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana"><a href="Class%20Documentation/html/functions.html" target="_top"><span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana">Class
member of Bayes++</span></a></span></div>
				<span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana">
					<div><span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana"><a href="Class%20Documentation/html/files.html" target="_top"><span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana">File
List of    Bayes++</span></a></span></div>
					<span style="FONT-WEIGHT: normal; FONT-SIZE: 18pt; FONT-FAMILY: Verdana"></span>
				</span>
			</span></blockquote>
		<h3 align="left"><a href="Bayes++.htm">Back to Bayes++ Download Page</a></h3>
		<h1>Introduction</h1>
		<p>Bayesian Filtering is a probabilistic technique for data fusion. The techniques 
			result from using a concise mathematical formulation to represent the state of 
			a system. Probabilities are used to represent states, likelihood functions to 
			represent their relationships. In this form Bayes rule can be applied and 
			further related probabilities deduced.</p>
		<p>The Bayesian formulation results in a naturally iterative solution to data 
			fusion and particularly to filtering problems. For linear dynamic systems, 
			discrete solutions such as the Kalman filter apply.</p>
		<p>Bayes++ is a library of C++ classes. These classes represent and implement a 
			wide variety of numerical estimation algorithms for Bayesian Filtering. The 
			classes provide tested and consistent numerical methods and the class hierarchy 
			explicitly represents the variety of filtering algorithms and model types.</p>
		<p>The following documentation summarises the classes available and provides some 
			general comments on their use. Each class’s <b>.h</b> file provides a complete 
			interface specification and the numerical form of the solution. Implementation 
			issues are documented in the <b>.cpp</b> files.</p>
		<h2>Numerical Schemes in Bayes++</h2>
		<p>There is a wide range of different numerical techniques for filtering. For 
			example the Kalman filter (a linear estimator which calculates the 2<sup>nd</sup>
			order statistics of the system) is represented using either a state vector and 
			a covariance matrix, or alternatively in information form.</p>
		<p>Each numerical technique is a <b><i>Scheme</i></b>, and each Scheme is 
			implemented as a class. Each scheme implements the algorithms required by a 
			filter’s particular numerical form. The schemes provide a common interface that 
			allows:</p>
		<p>
			i.&nbsp;initialisation and update of the state, and</p>
		<p>ii. predict and observe functions with parameterised models</p>
		<p>Each Scheme has both advantages and disadvantages. The numerical complexity and 
			efficiency varies, as does the numerical stability. The table below lists all 
			Schemes together with any requirement on the representation associated with the 
			algorithm.</p>
		<div>
			<table cellspacing="0" cellpadding="0" width="725" border="5">
				<tbody>
					<tr>
						<td valign="top" width="30%">
							<b><font face="Verdana" size="5">Scheme</font></b>
						</td>
						<td valign="top" width="43%">
							<b><font face="Verdana" size="4">Formulation and<br>
									Algorithm used</font></b>
						</td>
						<td valign="top" width="25%">
							<b><font face="Verdana" size="4">Representation Requirements</font></b>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>Covariance_filter</h4>
							<p><span><font face="Verdana">Extended Kalman Filter (EKF)</font></span></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[6]</span></a></span></p>
						</td>
						<td valign="top" width="43%">
							<p><span><font face="Verdana">Classic 2<sup>nd</sup> order filter with state mean and 
										covariance representation. Non-linear models require a gradient linearisation 
										(Jacobian). Uses the common innovation update formulation.</font></span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">Innovation covariance invertible</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>Unscented_filter</h4>
							<p><span><font face="Verdana">Kalman filter using unscented non-linear approximations</font></span></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[1]</span></a><font face="Verdana"> </font>
								</span></p>
						</td>
						<td valign="top" width="43%">
							<p><span><font face="Verdana">Unscented non-linear transformations replace the Jacobians 
										used in the EKF- reducing linearisation errors in all cases.</font></span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">Innovation covariance invertible</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>Iterated_covariance_filter</h4>
							<p><span><font face="Verdana">Modified EKF update</font></span></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[6]</span></a></span></p>
						</td>
						<td valign="top" width="43%">
							<p><span><font face="Verdana">Kalman filter for highly non-linear observation models. 
										Observation update is iterated using an Euler approximation.</font></span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">State, observation and innovation covariance invertible</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>Information_filter</h4>
							<p><font face="Verdana">Extended Information Filter (EIF)</font></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[7]</span></a></span></p>
						</td>
						<td valign="top" width="43%">
							<p><span><font face="Verdana">Inverse Covariance or Information form filter. Non-linear 
										prediction requires conversion back to state representation. Update, directly 
										on information using modified innovation formulation.</font></span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">State, observation and innovation covariance invertible</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>Information_joseph_filter</h4>
							<p><span><font face="Verdana">EIF with linear prediction</font></span></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[7]</span></a></span></p>
						</td>
						<td valign="top" width="43%">
							<p><font face="Verdana">Invertible <u>linear</u> model allows direct information form 
									prediction - avoids EIF conversion back to state and covariance.</font></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">Joseph prediction, observation and innovation covariance 
										invertible</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>Information_root_filter</h4>
							<p><span><font face="Verdana">Square root information filter (SRIF)</font></span></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[4]</span></a></span></p>
						</td>
						<td valign="top" width="43%">
							<p><span><font face="Verdana">Numerically stable solution using factorisation of inverse 
										covariance as R<sup>T</sup>R</font></span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">Invertible prediction model. Prediction and observation 
										covariance invertible</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>UD_filter</h4>
							<p><span><font face="Verdana">Square root covariance filter</font></span></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[2]</span></a></span></p>
						</td>
						<td valign="top" width="43%">
							<p class="MsoNormal"><span style="FONT-FAMILY: Veranda"><font face="Verdana">Numerically 
										stable solution using UdU<sup>T</sup> factorisation of covariance. </font>
								</span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">Innovation covariance invertible. Linearized observation 
										requires uncorrelated noise</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>SIR_filter</h4>
							<p><span><font face="Verdana">Sequential Importance Re-sampling filter</font></span></p>
							<p><span><a href="#References"><span style="FONT-FAMILY: Verdana">See
       Reference[5]</span></a></span></p>
						</td>
						<td valign="top" width="43%">
							<p><span><font face="Verdana">A bootstrap representation of the state distribution – no 
										linear or Gaussian assumptions required.</font></span></p>
							<p><span><font face="Verdana">Uncorrelated linear roughening of samples.</font></span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">Bootstrap samples collapse to a degenerate from with fewer 
										samples then states.</font></span></p>
						</td>
					</tr>
					<tr>
						<td valign="top" width="30%">
							<h4>SIR_kalman_filter</h4>
							<p><span><font face="Verdana">Sequential Importance Re-sampling filter</font></span></p>
							<p><span><font face="Verdana"></font></span>
							</p>
						</td>
						<td valign="top" width="43%">
							<p><span><font face="Verdana">A bootstrap representation of the state distribution – no 
										linear or Gaussian assumptions required.</font></span></p>
							<p><span><font face="Verdana">Additional computes state mean and covariance. Correlated 
										linear roughening of samples. </font>
								</span></p>
						</td>
						<td valign="top" width="25%">
							<p><span><font face="Verdana">As above. Correlation non-computable from degenerate samples.</font></span></p>
							<p><span><font face="Verdana"></font></span>
							</p>
						</td>
					</tr>
				</tbody>
			</table>
		</div>
		<p><font face="Verdana"></font>
		</p>
		<h1>Dual Abstraction</h1>
		<p>
			The aim of Bayes++ is to provide a powerful and extensible structure for 
			Bayesian filtering. This is achieved by hierarchical composition of related 
			objects. In C++ these are represented by a hierarchy of polymorphic classes. 
			Numerical Schemes are grouped by their state representation. The Schemes 
			themselves provided the filtering operations on these representations.</p>
		<p>To complete a filter, an associated prediction and observation model must also 
			be specified. These model classes parameterise the filtering operations. The 
			users responsibility is to chose a model type and provide the numerical 
			algorithm for that model. Again model classes composed using a hierarchy of 
			polymorphic abstract classes to represents the structure of each model type.</p>
		<p>This <em>dual abstraction</em>, separating numerical schemes from model 
			structure, provides a great deal of flexibility. In particular it allows a 
			well-specified model to be used with a variety of numerical schemes.</p>
		<h2>Filter Hierarchy</h2>
		<p>The table below lists a few of the key abstract filter classes upon which filter 
			Schemes are build. The C++ class hierarchy documentation gives the complete 
			structure.
		</p>
		<div align="center">
			<table cellspacing="0" cellpadding="0" width="455" align="center" border="4">
				<tbody>
					<tr>
						<td valign="top" width="35%"><strong>Bayes_filter</strong>
						</td>
						<td valign="top" width="65%">
							Base class for everything<br>
							(contains no data)
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Likelihood_filter</strong></td>
						<td valign="top" width="65%">Abstract filtering property<br>
							Represents only the Bayesian Likelihood of a state observation
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Functional_filter</strong></td>
						<td valign="top" width="65%">
							Abstract filtering property<br>
							Represents only the filter prediction by a simple functional (non-stochastic) 
							model
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>State_filter</strong></td>
						<td valign="top" width="65%">Abstract filtering property<br>
							Represents only the filter state and an update on that state</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Kalman_filter</strong>
						</td>
						<td valign="top" width="65%">
							Kalman representation of state.<br>
							Represents the filter as a state vector and a covariance matrix. That is the 
							1st (mean) and 2nd (covariance) moments of a distribution.
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Linrz_filter</strong>
						</td>
						<td valign="top" width="65%">
							Base class for a linear or gradient linearized Kalman filters.<br>
							Specifies filter operation using predict and observe functions
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Sample_filter</strong></td>
						<td valign="top" width="65%">Base class for filters representing state probability 
							distribution by a discrete sampling</td>
					</tr>
				</tbody>
			</table>
		</div>
		<h2 align="left">Model Hierarchy</h2>
		<p align="left">These two tables show some of the classes in the hierarchy upon 
			which models are built.</p>
		<div align="center">
			<table id="Table1" height="172" cellspacing="0" cellpadding="0" width="492" align="center" border="4">
				<tbody>
					<tr>
						<td valign="top" width="35%"><strong>Sampled_predict_model</strong></td>
						<td valign="top" width="65%">Sampled stochastic prediction model</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Functional_predict_model</strong>
						</td>
						<td valign="top" width="65%">Functional (non-stochastic) prediction model
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Addative_predict_model</strong></td>
						<td valign="top" width="65%">
							Additive Gaussian noise prediction model.<br>
							This fundamental model for linear/linearized filtering, with noise added to a 
							functional prediction
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Linrz_predict_model</strong></td>
						<td valign="top" width="65%">Linearized prediction model with Jacobian of 
							non-linear functional part</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Linear_predict_model</strong></td>
						<td valign="top" width="65%">Linear prediction model</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Linear_invertable_predict_model</strong></td>
						<td valign="top" width="65%">Linear prediction model which is invertible</td>
					</tr>
				</tbody>
			</table>
		</div>
		<div align="center">&nbsp;</div>
		<div align="center">
		</div>
		<div align="center">
			<table id="Table2" height="137" cellspacing="0" cellpadding="0" width="494" align="center" border="4">
				<tbody>
					<tr>
						<td valign="top" width="35%" height="42">
							<strong>Likelihood_observe_model</strong>
						</td>
						<td valign="top" width="65%" height="42">
							Likelihood observation model<br>
							The most fundamental Bayesian definition of an observation
						</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Functional_observe_model</strong></td>
						<td valign="top" width="65%">Functional (non-stochastic) observation model</td>
					</tr>
					<tr>
						<td valign="top" width="35%"><strong>Linrz_uncorrelated_observe_model</strong></td>
						<td valign="top" width="65%">Linearized observation model with Jacobian of 
							non-linear functional part and additive uncorrelated noise</td>
					</tr>
					<tr>
						<td valign="top" width="35%" height="26"><strong>Linrz_correlated_observe_model</strong></td>
						<td valign="top" width="65%" height="26">as above, but with additive correlated 
							noise</td>
					</tr>
				</tbody>
			</table>
		</div>
		<h1>Three Examples</h1>
		<h2>Simple Example (Initialise – Predict – Observe)</h2>
		<p>This is very simple example for those who have never used the Bayesian Filtering 
			Classes before. If you wish, <a href="simpleExample.cpp">View the Source</a> online.
		</p>
		<p>The example shows how two classes are built. The first is the prediction model, 
			the second the observation model. In this example they represent a simple 
			linear problem with only one state variable and constant model noises.</p>
		<p>The example then constructs a filter. The <b>Unscented</b> filter scheme is 
			chosen to illustrate how this works, even on a simple linear problem. After 
			construction the filter is given the problem’s initial conditions. A prediction 
			using the predefine prediction model is then made. This prediction is then 
			fused with an external observation given by the example and the defined 
			observation model. At each stage, the filter’s state estimate and variance 
			estimate are printed.</p>
		<h2>Position and Velocity</h2>
		<p>This example solves a simple position and velocity observation problem using the 
			Bayesian filter classes. The system has two states, position and velocity, and 
			a noisy position observation is simulated. Two variants:</p>
		<p>1) direct filter</p>
		<p>2) indirect filter where the filter is preformed on error and state is estimated 
			indirectly</p>
		<p>are demonstrated using a specified numerical scheme. The example shows how to 
			build <u>linear</u> prediction and observation models and to cycle the filter 
			to produce state estimates.</p>
		<h2>Quadratic Calibration</h2>
		<p>This example implements a “Quadratic Calibration” filter. The observer is trying 
			to estimate the state of a system as well as the systems scale factor and bias. 
			A simple system is simulated where the state is affected by a known 
			perturbation.</p>
		<p>The example shows how to build <u>linearized</u> prediction and observation 
			model and cycle a filter to produce state estimates.</p>
		<h1>Capabilities</h1>
		<h2>Linear models</h2>
		<p>Many of the filters use classical linear estimation techniques, such as the 
			Kalman filter. To make them useful they are applied in modified forms to cope 
			with <u>linearized</u> models of some kind. They are commonly extended, using a 
			gradient linearized model to update uncertainty, while the state is directly 
			propagated through the non-linear model. This is the <i>Extended </i>form used 
			by the EKF.</p>
		<p>However, some numerical schemes cannot be modified using the <i>Extended</i> form. 
			In particular, it is not always possible to use the extended form with 
			correlated noises. Where this is the case, the linear form of the model must be 
			used.</p>
		<p>There are also many Bayesian filters that work with non-Gaussian noise and 
			non-linear models - such as the <i>SIR_filter</i>. The SIR scheme has been 
			built so it works with Likelihood model.
		</p>
		<p>The filters support discontinuous models such as those depending on angles. In 
			this case the model must be specifically formulated to normalise the states. 
			However, some schemes need to rely on an additional normalisation function. 
			Normalisation works well for functions that are locally continuous after 
			normalisation (such as angles). Non-linear functions that cannot be made into 
			locally continuous models are not appropriate for linear filters.</p>
		<h2>Interface Regularity</h2>
		<p>Where possible, the Schemes have been designed to all have the same interface, 
			providing for easy interchange. However, this is not possible in all cases as 
			the mathematical methods vary greatly. For efficiency some schemes also 
			implement additional functions. The functions can be use to avoid 
			inefficiencies where the general form is not required.</p>
		<p>Scheme class constructors are irregular (their parameter list varies) and must 
			be used with care. Each numerical scheme has different requirements, and the 
			representation size needs to be parameterised.</p>
		<h3>Open interface</h3>
		<p>The design of the class hierarchy is deliberately open. Many of the variables 
			associated with schemes are exposed as <i>public members</i>. For example the 
			covariance filter’s innovation covariance is public. This is to allow efficient 
			algorithms to be implemented using the classes. In particular it is often the 
			case that subsequent computations reuse the values that have already been 
			computed by the numerical schemes.</p>
		<p>Furthermore many temporaries are <i>protected members</i> to allow derived 
			classes to modify a scheme without requiring any additional overhead to 
			allocate its own temporaries.</p>
		<p>Open interfaces are potentially hazardous. The danger is that abuse could result 
			in unnecessary dependencies on particular implementation characteristics.</p>
		<h2>Numerical and Exception Guarantees</h2>
		<p>It is important to be able to rely on the numerical results of Bayes++. 
			Generally the Schemes are implemented using the most numerically stable 
			approach available in the literature. Bayes++ provides its own UdU' 
			factorisation functions to factorise PSD positive (semi)definite matrices. The 
			factorisation is numerically stable, computing and checking the conditioning of 
			matrices.</p>
		<p dir="ltr" style="MARGIN-RIGHT: 0px">
			Exceptions are thrown in the case of numerical failure. They follow a simple 
			rule:<br>
			<em><strong>Bayes_filter_exception</strong> is thrown if an algorithm cannot 
				continue or guarantee the numerical post-conditions of a function</em></p>
		<p>The only <em>exception guarantee</em> that Bayes++ makes when throwing <strong><em>Bayes_filter_exception</em>
			</strong>from any functions, is that no resources will be leaked. Therefore, 
			unless otherwise specified, the numeric value of a class's public state is <strong>undefined</strong>
			after this exception is thrown by a member of a class.</p>
		<p>Be aware that numerical post-conditions may be met even with extreme input 
			values. For example some inputs may result in overflow of a result. This does 
			not invalidate the post-conditions that the result is positive. Even some <em>Not a 
				Number</em> values may be valid. Therefore the results may be computed 
			without exception, but include <em>NaN</em> or <em>non-number</em> values.</p>
		<h3>Using exceptions in filter schemes</h3>
		<p>
			What does this mean when using a filter Scheme? If the Scheme throws an 
			exception then, unless otherwise specified, the numeric value of its state is 
			undefined. Therefore you <strong>cannot</strong> use any function of the filter 
			that has any preconditions on its numerical state. To continue using a filter 
			either the <em>init</em>() function should be used (which has no 
			pre-conditions) or the filter destructed.</p>
		<p>Generally to access the public state of a Scheme you must first call the <em>update</em>() 
			function. If no exception is thrown then the post-conditions of update are 
			guaranteed. The post-conditions of <em>update</em>() may include such useful 
			properties as the covariance matrix is PSD. Many application specific tests may 
			also be required. Just being PSD doesn't say much about X. It could even have <em>Infinity</em>
			values on the diagonal! Conditioning, trace, determinate, and variance bounds 
			can all be applied at his point.
		</p>
		<h1>Rational</h1>
		<h2>Polymorphic design</h2>
		<p>Why are models and filter Schemes <strong>polymorphic</strong> ? OR why are they 
			implemented with virtual functions?</p>
		<blockquote dir="ltr" style="MARGIN-RIGHT: 0px">
			<p>One alternative would be to use <strong>generic</strong> instead of <strong>polymorphic</strong>
				Schemes. This would implement Schemes as templates. If the sizes of matrix 
				operations could be fixed generic models and schemes would directly manipulate 
				matrices for predefined size.</p>
		</blockquote>
		<p>Bayes++ relies on a <strong>dual abstraction</strong>, separating numerical 
			schemes from model structure. To represent these as <strong>polymorphic hierarchies</strong>
			was a very important decision in the design of Bayes++. After a fair bit of use 
			I still think it was the correct one. Why?</p>
		<p>A) <em>Usage</em> : There are many (run time) polymorphic things I really want 
			to do with the library. I want to be able to build composite filtering 
			algorithms that switch models and schemes at run time. This requires BOTH 
			polymorphic filters and models AND runtime sizing of matrices.<br>
			A generic approach especially one parametised on matrix size would not allow 
			this. The code size produced would also bloat massively. Not even STL 
			parameterises its containers on size!</p>
		<p>B) <em>Type safety</em>: There is surprising little orthoginality in filtering 
			schemes. The numerics often dictate restrictions or additional functionality. 
			The type hierarchy in Bayes++ provides a succinct and safe way to enforce much 
			of this structure.<br>
			In a Generic approach checking that a particular Scheme models a generic 
			concept correct would be very difficult to enforce.</p>
		<p>C) <em>Compiler problems</em>: Generic programming in C++ has a nasty syntax, is 
			very slow to compile and pushes the limits of compiler reliability.</p>
		<p>D) <em>Efficiency</em>: The run time overhead of a polymorphic filter is 
			negligible compared to the matrix algebra required. In fact using common code 
			may increase efficiency on a many computing architectures.</p>
		<h1 style="TEXT-ALIGN: left" align="left">Matrix Representation</h1>
		<p>Two modern C++ libraries can be used as the representation for matrices (and 
			vectors) in Bayes++: <A href="http://www.genesys-e.org/ublas/index.htm">uBLAS</A>
			and&nbsp;the&nbsp;<a href="http://www.osl.iu.edu/research/mtl/" target="_top"><span style="FONT-FAMILY: Verdana"> Matrix Template Library</span></a>
			. Previous versions of the filters were implemented for <i>MPP</i> and <i>TNT</i>
			but are no longer supported. Both are efficient and portable;&nbsp;their 
			designs include the matrices representations and linear algebra operations 
			required by Bayes++.</p>
		<p>
			The Bayes++ implementation uses a set of adaptor classes in namespace <i>Bayesian_filter_matrix</i>
			(FM for short) for matrix access. This system should make it simpler to port 
			the library to other matrix types. They support a common syntax with the 
			following features.</p>
		<UL>
			<LI>
			Matrix row/column access with operator[]. Bayes++ implements many functions 
			using row operations.
			<LI>
			Overloading of operator* or prod() for matrix multiplication. The former may be 
			removed to give better control of temporary creation and expression complexity.
			<LI>
				For efficiency when implementing with&nbsp;MTL a few specific functions such as 
				add(a,b,c), rank_one_update etc</LI></UL>
		<P>For uBLAS these function generally have no runtime overhead. This efficiency is 
			due to uBLAS's excellent expression template implementation.&nbsp;For 
			MTL&nbsp;most functions&nbsp;are reasonably&nbsp;efficient,&nbsp;but some 
			functions&nbsp;create matrix temporaries to return values.</P>
		<h2>Matrix library efficiency - Temporary object usage</h2>
		<p>
			Most of the filters avoid using Matrix and Vec temporaries. They also have 
			optimisations for the fixed size of state, noise and observations. In 
			particularly the UD_filter scheme avoids creating any temporary objects. All 
			matrices maintain a fixed size, other then when the observation state size is 
			varied.
		</p>
		<p>
			Why are temporary matrix objects bad? The main difficulty is construction and 
			destruction of Matrices. This generally requires dynamic memory allocation 
			which is very slow. For small matrices this dominates compared to the cost of 
			simple operations such as addition and multiplication.
		</p>
		<p>There are three ways out of this problem.
		</p>
		<blockquote dir="ltr" style="MARGIN-RIGHT: 0px">
			<p>1. Use fixed size matrices; they can (nearly) always be efficiently allocated on 
				the stack.<br>
				2. Use stack based dynamic allocators (such as C99's dynamic arrays or alloca) 
				for temporaries.<br>
				3. Don't create temporaries. This is achievable with a combination of hard 
				coding in the algorithms (pre-allocation) and by using a Matrix library with 
				expression templates to avoid creating temporaries for return values.</p>
		</blockquote>
		<p>
			Bayes++ is implemented to avoid creating temporaries. At present my solution is 
			somewhat of a compromise. MTL is only moderately efficient on most C++ 
			compilers. On release code this results in a 50% performance reduction if 
			temporaries are avoided. uBLAS attains close to optimal efficiency in many 
			situations.</p>
		<P>The UD_filter is a good illustration. It has been hand crafted to avoid 
			construction of any temporaries (I use it for embedding) and can be compiled 
			with either uBLAS, MTL or a special fast matrix library. UD_filter is heavily 
			optimised to use row operations to avoid indexing overheads. uBLAS and the 
			special library achieve comparable results,&nbsp;which I believe are as fast as 
			can be achieved.</P>
		<p>On the flip side, the current Unscented_filter implementation shows some of the 
			problems. Because of my wrapper classes Row/Column of a 2D matrix are not 
			compatible with the Vec type. This results in a lot of unnecessary copying into 
			pre-allocated temporary vectors.</p>
		<p>Future work includes a general method of avoiding implementation 
			temporaries.&nbsp;Filter schemes will be parameterised with helper objects that 
			deal with all the temporaries required. These helper objects can then hide all 
			the hard work of pre-allocation or dynamic allocation of temporary objects.
		</p>
		<h1>Restrictions</h1>
		<p>For numerical precision,<i> </i>all filter calculations use <b>doubles</b>. 
			Templates could be used o provide numerical type parameterisation but are not. 
			However the base class <i>Bayes_filter</i> defines <i>Float,</i> which is used 
			throughout as the numerical type.
		</p>
		<p>The <i>Bayes_filter</i> base class is constructed with constant state size. This 
			allows efficient use of the matrix library to implement the algorithms. Each 
			derived class requires additional representation and matrix temporaries and 
			where possible they are restricted to a constant size. In general, the only 
			unknown is the size of the observation state as parameterised by the 
			observation model.</p>
		<p>Where the state size varies efficient solutions are still possible using either 
			spares matrix representations or by recreating new filters with increased size.
		</p>
		<h1 style="TEXT-ALIGN: left" align="left">Portability</h1>
		<p>Bayes++ only makes use of ISO standard C++. The source code uses moderately 
			advance C++ constructs but only restricted use of the C++ template system. 
			Therefore with a few minor syntactic alterations it should compile with any 
			modern C++ compiler.</p>
		<p>Visual C++ 6 and Visual C++ 7 (.NET) are used to compile and test Bayes++. It is 
			occasionally checked and works fine with GCC 3.x. It will compile with GCC 
			2.95.2 but with some modifications to the class declarations that contain the 
			line 'using Linrz_filter'.
		</p>
		<p><strong>MTL</strong> works for these compilers, but the download
			<span style="FONT-SIZE: 12pt; FONT-FAMILY: 'Times New Roman'">of  version 2.1.2-20</span>
			available is <strong>not</strong> up to date and contains some major errors. If 
			you do not wish to apply many patches yourself <a href="Bayes++.htm">download my 
				patched ZIP archive for the Bayes++</a>. For compilers other then Visual 
			C++ the mtl_config.h may have to be edited or recreated with autoconf.</p>
		<p>The Position Velocity and Quadratic Calibration examples use the <a href="http://www.boost.org/">
				<span style="FONT-FAMILY: Verdana">http://www.boost.org</span></a> library 
			for random number generation.</p>
		<h1 style="TEXT-ALIGN: left" align="left">The future</h1>
		<p>The Bayesian Filtering Classes are now a stable set of solution for the majority 
			of linear, linearisable and non-linear filtering problems. I am very happy with 
			their form and function. In particular, the two-layer abstraction works 
			extremely well. The classes show best practice in both their design and in 
			efficiency and numerical stability of the algorithms used. So where to go from 
			here?</p>
		<p>The basic tenant of Bayesian theory is that the Likelihood function complete 
			captures all that is know to update the state. The Bayesian Filtering classes 
			now allow the modelling systems using Likelihood functions. At present the SIR 
			filter is the only Likelihood filter. Sample filters will grow into a separate 
			branch in the class hierarchy. A general set of adaptors will be required to 
			move between these varied representations.
		</p>
		<p>To further improve the abstraction, the method of internal variable exposure 
			needs to be regularised. This will require the addition of a few classes that 
			hold and limit access to filters internal variables.</p>
		<p>The ordering of parameters&nbsp;used in&nbsp;Scheme class constructors&nbsp;is 
			prone to error. It requires the parameter list to be varied for each scheme 
			used. An extensible method of model parameterisation is required. A common 
			parameterisation could then be used with scheme constructor extracting the 
			information they require.</p>
		<p>At present MTL does have a few drawbacks! In MTL assignment is by reference. 
			This means MTL objects do not have value
			<SPAN style="FONT-SIZE: 12pt; FONT-FAMILY: 'Times New Roman'; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-AU; mso-fareast-language: EN-AU; mso-bidi-language: AR-SA">semantics</SPAN>. 
			I believe this is a fatal flaw in its design and causes many problems. I 
			believe the design of uBLAS is more elegant and better constructed. Its 
			expression template system works well. The future of Bayes++ probably is to use 
			uBLAS as the matrix library.</p>
		<h2>STATE ABSTRACTION</h2>
		<p>
			Can the state representation be abstracted away from the numerics of a filter 
			Scheme?</p>
		<p>At the moment Bayes++ filter Schemes are a combination of the numerical solution 
			(e.g. innovation update) AND the representation (e.g. Covariance matrix). Can 
			these two functions be separated?</p>
		<p>A highly sophisticated solution could use polymorphic (or generic) filter 
			algorithms that depend on the type (or traits) of the representation. I think 
			this in unnecessarily complex. In Bayes++ a Scheme should only implement one 
			algorithm.</p>
		<p>The problem I would like to address is a bit more limited. A lot of 
			representations are naturally hierarchical and dynamic.<br>
			e.g. state := vehicle states &amp; feature states, feature states := {feature1 
			state &amp; feature2 state ....}</p>
		<p>There would seem to be too possible ways to solve this</p>
		<p>A) HARD: Use a state representation that represents this kind of hierarchy. The 
			filter schemes could therefore use hierarchical numerical solutions that 
			exploit the properties of the hierarchical state.</p>
		<p>
			B) EASY: Allow a sparse matrix representation of state. If the sparse 
			representation is a Graph then the sort of augmented state representation in 
			the example can be easily built without any copy overhead. Each scheme would 
			then just use sparse matrix techniques in its numerical solution. Mostly what 
			is needed is Cholesky and QR decomposition and these have good sparse 
			solutions. Obviously there would be a runtime overhead for this but it would be 
			great for
			<span style="FONT-SIZE: 12pt; FONT-FAMILY: 'Times New Roman'">Simultaneous  </span>
			Location and Mapping problems!
		</p>
		<h1 style="TEXT-ALIGN: left" align="left"><a name="References"></a>References</h1>
		<p>[1] “A New Approach for Filtering Nonlinear Systems”, SJ Julier JK Uhlmann HF 
			Durrant-Whyte, American Control Conference 1995</p>
		<p>[2] "Factorisation Methods for Discrete Sequential Estimation", Gerald J. 
			Bierman, ISBN 0-12-097350-2</p>
		<p>[3] "Real time Kalman Filter Application", Mohinder S. Grewal, Angus P. Andrews, 
			ISBN 0-13-211335-X</p>
		<p>[4] "Extension of Square-Root Filtering to Include Process Noise", P. Dyer and 
			S. McReynolds, Journal of Optimization Theory and Applications, Vol.3 No.6 1969</p>
		<p>[5] "Novel approach to nonlinear-non-Guassian Bayesian state estimation", NJ 
			Gordon, DJ Salmond, AFM Smith , IEE Proceeding-F, Vol.140 No.2 April 1993</p>
		<p>[6] “Tracking and Data Association”, Y Bar-Shalom and T Fortmann, Academic 
			Press, ISBN 0-12-079760-7</p>
		<p>[7] “Stochastic Models, Estimation, and Control”, Peter S Maybeck, Academic 
			Press, ISBN 0-12-480701-1</p>
		<p>[8] "Exception-Safety in Generic Components", David Abrahams, Proc. of a 
			Dagstuhl Seminar 'Generic Programming', Lecture Notes on Computer Science 1766, <a href="http://www.boost.org/more/error_handling.html" target="_top">
				http://www.boost.org/more/error_handling.html</a></p>
		<br>
	</body>
</html>
