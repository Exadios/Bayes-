<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
  <title>Bayesiean Filtering Classes</title>
                    
  <link media="all" href="http://www.acfr.usyd.edu.au/style.css"
 type="text/css" rel="Stylesheet">
</head>
  <body>
     
<h1 align="center"><u>Bayesian Filtering Classes</u></h1>
         
<div>     
<h3><a href="Bayes++.htm">Bayes++ Overview</a></h3>
   </div>
         
<h1>Introduction</h1>
         
<p>Bayesian Filtering is a probabilistic technique for data fusion. The technique 
  combines a concise mathematical formulation of a system with observtions 
 of that system. Probabilities are used to represent the state of a system, 
 likelihood functions to represent their relationships. In this form Bayesian 
 inference can be applied and further related probabilities deduced. See <a
 href="http://www.wikipedia.org/">Wikipedia</a> for information on <a
 href="http://www.wikipedia.org/wiki/Probability_theory">Probability theory</a>,
  <a href="http://www.wikipedia.org/wiki/Bayes%27_theorem">Bayes theorem</a>,
  <a href="http://www.wikipedia.org/wiki/Bayesian_inference">Bayesian Inference</a>.</p>
         
<p>For <u>discreate</u> systems the Bayesian formulation results in a naturally 
  iterative data fusion solution. For <u>dynamic</u> systems there is a class 
  of solutions, discreate <u>filters</u>, that combine observed inputs to 
the system with the dynamic model. A filter that iteratively incorperates 
new information is an <u>estimator</u>. For linear dynamic systems, discrete 
solutions such as the Kalman filter apply.</p>
         
<p>Bayes++ is a library of C++ classes. These classes represent and implement 
  a wide variety of numerical algorithms for Bayesian Filtering of discreate 
  systems. The classes provide tested and consistent numerical methods and
  the class hierarchy explicitly represents the variety of filtering algorithms 
  and system model types.</p>
           
<p>The following documentation summarises the classes available and provides 
  some general comments on their use. Each class's <b>.hpp</b> file provides 
  a complete interface specification and the numerical form of the solution. 
  Implementation issues are documented in the <b>.cpp</b> files.</p>
           
<h2>Numerical Schemes in Bayes++</h2>
           
<p>There is a wide range of different numerical techniques for filtering. 
  For example the Kalman filter (a linear estimator which calculates the 
2<sup>nd</sup> order statistics of the system) is represented using either 
 a state vector and a covariance matrix, or alternatively in information 
form.</p>
           
<p>Each numerical technique is a <b><i>Scheme</i></b>, and each Scheme is 
   implemented as a class. Each scheme implements the algorithms required 
by  a filter's particular numerical form. The schemes provide a common interface 
  that allows:</p>
           
<p> i.&nbsp;initialisation and update of the state, and</p>
           
<p>ii.&nbsp;predict and observe functions with parameterised models</p>
           
<p>Each Scheme has both advantages and disadvantages. The numerical complexity 
  and efficiency varies, as does the numerical stability. The table below 
 lists all Schemes together with any requirement on the representation associated 
  with the algorithm.</p>
           
<div>      
<table cellspacing="0" cellpadding="0" width="725" border="5">
      <tbody>
      <tr>
      <td valign="top" width="30%"> <b><font face="Verdana" size="5">Scheme</font></b> 
        </td>
      <td valign="top" width="43%"> <b><font face="Verdana" size="4">Formulation 
 and<br>
      Algorithm used</font></b> </td>
      <td valign="top" width="25%"> <b><font face="Verdana" size="4">Representation 
 Requirements</font></b> </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
                                 
      <h4>Covariance_filter</h4>
                                                                        
   
      <p>Extended Kalman Filter (EKF)</p>
                                              
      <p><a href="#References">See Reference[6]</a></p>
      </td>
      <td valign="top" width="43%">                                     
   
      <p>Classic 2nd order filter with state mean and   covariance representation. 
  Non-linear models require a gradient linearisation (Jacobian). Uses the 
 common innovation update formulation.</p>
      </td>
      <td valign="top" width="25%">                                     
   
      <p>Innovation covariance invertible</p>
      </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
   
      <h4>Unscented_filter</h4>
                                              
      <p>Kalman filter using unscented non-linear approximations</p>
                                              
      <p><a href="#References">See Reference[1]</a></p>
      </td>
      <td valign="top" width="43%">                                     
   
      <p>Unscented non-linear transformations replace the Jacobians used 
in the EKF- reducing linearisation errors in all cases.</p>
      </td>
      <td valign="top" width="25%">                                     
   
      <p>Innovation covariance invertible</p>
      </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
   
      <h4>Iterated_covariance_filter</h4>
                                                                        
   
      <p>Modified EKF update</p>
                                                                        
   
      <p><a href="#References">See Reference[6]</a></p>
      </td>
      <td valign="top" width="43%">                                     
                                 
      <p>Kalman filter for highly non-linear observation models. Observation 
  update is iterated using an Euler approximation.</p>
      </td>
      <td valign="top" width="25%">                                     
                                 
      <p>State, observation and innovation covariance invertible</p>
      </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
                                 
      <h4>Information_filter</h4>
                                                                        
   
      <p>Extended Information Filter (EIF)</p>
                                                                        
   
      <p><a href="#References">See Reference[7]</a></p>
      </td>
      <td valign="top" width="43%">                                     
                                 
      <p>Inverse Covariance or Information form filter. Non-linear prediction 
  requires conversion back to state representation. Update, directly on information 
  using modified innovation formulation.</p>
      </td>
      <td valign="top" width="25%">                                     
                                 
      <p>State, observation and innovation covariance invertible</p>
      </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
                                 
      <h4>Information_joseph_filter</h4>
                                                                        
   
      <p>EIF with linear prediction</p>
                                                                        
   
      <p><a href="#References">See Reference[7]</a></p>
      </td>
      <td valign="top" width="43%">                                     
                                 
      <p>Invertible linear model allows direct information form prediction 
  - avoids EIF conversion back to state and covariance.</p>
      </td>
      <td valign="top" width="25%">                                     
                                 
      <p>Joseph prediction, observation and innovation covariance invertible</p>
      </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
                                 
      <h4>Information_root_filter</h4>
                                                                        
   
      <p>Square root information filter (SRIF)</p>
                                                                        
   
      <p><a href="#References">See Reference[4]</a></p>
      </td>
      <td valign="top" width="43%">                                     
                                 
      <p>Numerically stable solution using factorisation of inverse covariance 
 as RTR</p>
      </td>
      <td valign="top" width="25%">                                     
                                 
      <p>Invertible prediction model. Prediction and observation covariance 
 invertible</p>
      </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
                                 
      <h4>UD_filter</h4>
                                                                        
   
      <p>Square root covariance filter</p>
                                                                        
   
      <p><a href="#References">See Reference[2]</a></p>
      </td>
      <td valign="top" width="43%">                                     
                                 
      <p class="MsoNormal">Numerically stable solution using UdU' factorisation 
 of covariance.</p>
      </td>
      <td valign="top" width="25%">                                     
                                 
      <p>Innovation covariance invertible. Linearized observation requires 
 uncorrelated noise</p>
      </td>
      </tr>
      <tr>
           <td valign="top"><b>CI_filter<br>
           <br>
           </b>Covariance intersect filter                              
                         
      <p><a href="#References">See Reference[8]</a></p>
           </td>
           <td valign="top">CI is interesting as it provides a weaker but 
more   robust fussion then traditional covariance based method such as the 
Kalman   filter. It estimates state and an upper bound of what its covariance 
could   be.</td>
           <td valign="top">No solution if covariances don't intersect.<br>
           </td>
         </tr>
         <tr>
      <td valign="top" width="30%">                                     
                                 
      <h4>SIR_filter</h4>
                                                                        
   
      <p>Sequential Importance Re-sampling filter</p>
                                                                        
   
      <p><a href="#References">See Reference[5]</a></p>
      </td>
      <td valign="top" width="43%">                                     
                                 
      <p>A bootstrap representation of the state distribution &#8211; no linear 
  or Gaussian assumptions required.</p>
                                                                        
   
      <p>Uncorrelated linear roughening of samples.</p>
      </td>
      <td valign="top" width="25%">                                     
                                 
      <p>Bootstrap samples collapse to a degenerate from with fewer samples 
  then states.</p>
      </td>
      </tr>
      <tr>
      <td valign="top" width="30%">                                     
                                 
      <h4>SIR_kalman_filter</h4>
                                                                        
   
      <p>Sequential Importance Re-sampling filter</p>
                                          </td>
      <td valign="top" width="43%">                                     
                                 
      <p>A bootstrap representation of the state distribution &#8211; no linear 
  or Gaussian assumptions required.</p>
                                                                        
   
      <p>Additional computes state mean and covariance. Correlated linear 
  roughening of samples.</p>
      </td>
      <td valign="top" width="25%">                                     
                                 
      <p>As above. Correlation non-computable from degenerate samples.</p>
                                          </td>
      </tr>
                                    
  </tbody>      
</table>
   </div>
                  
<h1>Dual Abstraction</h1>
           
<p> The aim of Bayes++ is to provide a powerful and extensible structure
for Bayesian filtering. This is achieved by hierarchical composition of related 
 objects. In C++ these are represented by a hierarchy of polymorphic classes. 
 Numerical Schemes are grouped by their state representation. The Schemes 
themselves provided the filtering operations on these representations.</p>
           
<p>To complete a filter, an associated prediction and observation model must 
  also be specified. These model classes parameterise the filtering operations. 
  The users responsibility is to chose a model type and provide the numerical 
  algorithm for that model. Again model classes composed using a hierarchy 
  of polymorphic abstract classes to represents the structure of each model 
  type.</p>
           
<p>This <em>dual abstraction</em>, separating numerical schemes from model 
  structure, provides a great deal of flexibility. In particular it allows 
  a well-specified model to be used with a variety of numerical schemes.</p>
           
<h2>Filter Hierarchy</h2>
           
<p>The table below lists a few of the key abstract filter classes upon which 
  filter Schemes are build. The C++ class hierarchy documentation gives the 
  complete structure.</p>
           
<div align="center">      
<table cellspacing="0" cellpadding="0" width="455" align="center"
 border="4">
      <tbody>
      <tr>
      <td valign="top" width="35%"><strong>Bayes_filter</strong> </td>
      <td valign="top" width="65%"> Base class for everything<br>
      (contains no data)  </td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Likelihood_filter</strong></td>
      <td valign="top" width="65%">Abstract filtering property<br>
      Represents only the Bayesian Likelihood of a state observation</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Functional_filter</strong></td>
      <td valign="top" width="65%"> Abstract filtering property<br>
      Represents only the filter prediction by a simple functional (non-stochastic) 
   model</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>State_filter</strong></td>
      <td valign="top" width="65%">Abstract filtering property<br>
      Represents only the filter state and an update on that state</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Kalman_filter</strong></td>
      <td valign="top" width="65%"> Kalman representation of state statistic.<br>
      Represents a state vector and a covariance matrix. That is the 1st
(mean)   and 2nd (covariance) moments of a distribution.</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Linrz_filter</strong></td>
      <td valign="top" width="65%"> Base class for a linear or gradient linearized 
  Kalman filters.<br>
      Specifies filter operation using predict and observe functions</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Sample_filter</strong></td>
      <td valign="top" width="65%">Discreate reprensation of state statistic.<br>
     Base class for filters representing state probability distribution by
 a discrete sampling</td>
      </tr>
                                    
  </tbody>      
</table>
      </div>
           
<h2 align="left">Model Hierarchy</h2>
           
<p align="left">These two tables show some of the classes in the hierarchy 
  upon which models are built.</p>
           
<div align="center">      
<table id="Table1" height="172" cellspacing="0" cellpadding="0"
 width="492" align="center" border="4">
      <tbody>
      <tr>
      <td valign="top" width="35%"><strong>Sampled_predict_model</strong></td>
      <td valign="top" width="65%">Sampled stochastic prediction model</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Functional_predict_model</strong>
        </td>
      <td valign="top" width="65%">Functional (non-stochastic) prediction 
model    </td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Addative_predict_model</strong></td>
      <td valign="top" width="65%"> Additive Gaussian noise prediction model.<br>
      This fundamental model for linear/linearized filtering, with noise
added   to a functional prediction</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Linrz_predict_model</strong></td>
      <td valign="top" width="65%">Linearized prediction model with Jacobian
  of non-linear functional part</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Linear_predict_model</strong></td>
      <td valign="top" width="65%">Linear prediction model</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Linear_invertable_predict_model</strong></td>
      <td valign="top" width="65%">Linear prediction model which is invertible</td>
      </tr>
                                    
  </tbody>      
</table>
  </div>
  
<p>&nbsp;</p>
                         
<div align="center">      
<table id="Table2" height="137" cellspacing="0" cellpadding="0"
 width="494" align="center" border="4">
      <tbody>
      <tr>
      <td valign="top" width="35%" height="42"> <strong>Likelihood_observe_model</strong> 
   </td>
      <td valign="top" width="65%" height="42"> Likelihood observation model<br>
      The most fundamental Bayesian definition of an observation</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Functional_observe_model</strong></td>
      <td valign="top" width="65%">Functional (non-stochastic) observation
 model</td>
      </tr>
      <tr>
      <td valign="top" width="35%"><strong>Linrz_uncorrelated_observe_model</strong></td>
      <td valign="top" width="65%">Linearized observation model with Jacobian
  of non-linear functional part and additive uncorrelated noise</td>
      </tr>
      <tr>
      <td valign="top" width="35%" height="26"><strong>Linrz_correlated_observe_model</strong></td>
      <td valign="top" width="65%" height="26">as above, but with additive
 correlated    noise</td>
      </tr>
                                    
  </tbody>      
</table>
   </div>
           
<h1>Three Examples</h1>
           
<h2>Simple Example (Initialise &#8211; Predict &#8211; Observe)</h2>
           
<p>This is very simple example for those who have never used the Bayesian 
  Filtering Classes before. If you wish, <a href="simpleExample.cpp">View 
 the Source</a> online.</p>
           
<p>The example shows how two classes are built. The first is the prediction 
  model, the second the observation model. In this example they represent 
 a simple linear problem with only one state variable and constant model 
noises.</p>
           
<p>The example then constructs a filter. The <b>Unscented</b> filter scheme 
  is chosen to illustrate how this works, even on a simple linear problem. 
  After construction the filter is given the problem&#8217;s initial conditions. 
  A prediction   using the predefine prediction model is then made. This prediction
  is then fused with an external observation given by the example and the
 defined observation model. At each stage, the filter&#8217;s state estimate and
  variance estimate are printed.</p>
           
<h2>Position and Velocity</h2>
           
<p>This example solves a simple position and velocity observation problem 
  using the Bayesian filter classes. The system has two states, position and
 velocity, and a noisy position observation is simulated. Two variants:</p>
           
<p>1) direct filter</p>
           
<p>2) indirect filter where the filter is preformed on error and state is 
  estimated indirectly</p>
           
<p>are demonstrated using a specified numerical scheme. The example shows 
  how to build <u>linear</u> prediction and observation models and to cycle 
  the filter to produce state estimates.</p>
           
<h2>Quadratic Calibration</h2>
           
<p>This example implements a &#8220;Quadratic Calibration&#8221; filter. The observer 
  is trying to estimate the state of a system as well as the systems scale 
  factor and bias. A simple system is simulated where the state is affected 
  by a known perturbation.</p>
           
<p>The example shows how to build <u>linearized</u> prediction and observation 
  model and cycle a filter to produce state estimates.</p>
           
<h1>Capabilities</h1>
           
<h2>Probablistic representation of state</h2>
         
<p>Bayes rule is usualy defined in term of Probability Density Functions. 
  However PDFs never appear in Bayes++. They are always represented by their 
  statistics.</p>
         
<p>This is for good reason, there is very little that can be done algorithmically 
  with a function. However the sufficient statistics, given the assumptions 
  of a filter, can be easily manipulated to implement Bayes rule. Each filter 
  class is derived from a base classes that represent the statisitcs used. 
  For example the Kalman_filter and Sample_filter base classes.</p>
         
<p>It would be possible to use a common abstract base class for that enforces 
  the implementation of a PDF function; a function that maps state to probability. 
  This would provide a very weak method to view the PDF of state. However 
 such a function could not be efficiently implemented by all schemes. Therefore 
  to enforce this requirement in the base class would be highly restrictive.</p>
         
<h2>Linear and Linearized models</h2>
           
<p>Many of the filters use classical linear estimation techniques, such as 
  the Kalman filter. To make them useful they are applied in modified forms 
  to cope with <u>linearized</u> models of some kind. Commonly a   gradient 
  linearized model is used to update uncertainty, while the state is directly 
  propagated through the non-linear model. This is the <i>Extended</i> form 
  used by the Extended Kalman Filter.</p>
           
<p>However, some numerical schemes cannot be modified using the <i>Extended</i> 
  form. In particular, it is not always possible to use the extended form 
 with correlated noises. Where this is the case, the linearized model is 
used for uncertainty and state.</p>
           
<p>There are also many Bayesian filters that work with non-Gaussian noise 
  and non-linear models - such as the <i>SIR_filter</i>. The SIR scheme has 
  been built so it works with Likelihood model.</p>
           
<p>The filters support discontinuous models such as those depending on angles. 
  In this case the model must be specifically formulated to normalise the 
 states. However, some schemes need to rely on an additional normalisation 
  function. Normalisation works well for functions that are locally continuous 
  after normalisation (such as angles). Non-linear functions that cannot be
 made into locally continuous models are not appropriate for linear filters.</p>
           
<h2>Interface Regularity</h2>
           
<p>Where possible, the Schemes have been designed to all have the same interface, 
  providing for easy interchange. However, this is not possible in all cases 
  as the mathematical methods vary greatly. For efficiency some schemes also 
  implement additional functions. The functions can be use to avoid inefficiencies 
  where the general form is not required.</p>
           
<p>Scheme class constructors are irregular (their parameter list varies)
and must be used with care. Each numerical scheme has different requirements, 
  and the representation size needs to be parameterised. A template class
 Filter_scheme can be used to provide a generic constructor interface. It
provides provides specialisation for all the Schemes so they can be constructed
with a common parameter list.</p>
           
<h3>Open interface</h3>
           
<p>The design of the class hierarchy is deliberately open. Many of the variables 
  associated with schemes are exposed as <i>public members</i>. For example 
  the  covariance filter&#8217;s innovation covariance is public. This is to allow 
  efficient algorithms to be implemented using the classes. In particular 
 it is often the case that subsequent computations reuse the values that 
have already been computed by the numerical schemes. Each scheme definines 
 a <i>public resentation</i>.</p>
           
<p>Furthermore many temporaries are <i>protected members</i> to allow derived 
  classes to modify a scheme without requiring any additional overhead to
 allocate its own temporaries.</p>
           
<p>Open interfaces are potentially hazardous. The danger is that abuse could 
  result in unnecessary dependencies on particular implementation characteristics.
  </p>
         
<h4>Init and Update</h4>
         
<p>These two functions are defined in the filter class heirarchy.</p>
         
<blockquote>                    
  <p> Whenever the public representation of a scheme is changed (externally) 
  the filter should be <i>initialised</i> with <b>init</b>.</p>
                         
  <p>Whenever the public representation of a scheme is used the filter should 
  be <i>updated</i> with <b>update</b>.</p>
   </blockquote>
         
<h4>Assignment and Copy Construction</h4>
         
<p>Filter classes can be assigned when they are of the same size. In cases 
  where the class includes members in addition to the public representation 
  this is optimised so that only public representation is assigned. Assignment 
  is equvilent to <i>update</i>, assignment of public representation and <i>initialisation</i>
  from the new state.</p>
         
<p>Copy Constructors are NOT defined. Generally the classes are expensive 
  to copy and so copies should be avoided. Instead references, (smart) pointers, 
  assignment, init and update should combined to create copies if necessary.</p>
         
<h4>Resizing</h4>
         
<p>Classes assume their matrices maintain a constant size. They define this 
  on construction. The public representation should NOT be externally resized.</p>
         
<p> To resize a filter a new filter should be created from the old. The public 
  representation modified and then the new filter <i>initialised</i>.  Since 
 the definition of the schemes public representation varies, this avoids 
confusion as to how the newely sized filter is defined. </p>
         
<p>Since matrix resizing invariable involves reallocation and internal states 
  must be recomputed their is no efficiency lost in the above method.</p>
           
<h2>Numerical and Exception Guarantees</h2>
           
<p>It is important to be able to rely on the numerical results of Bayes++. 
  Generally the Schemes are implemented using the most numerically stable
 approach available in the literature. Bayes++ provides its own UdU' factorisation 
  functions to factorise PSD positive (semi)definite matrices. The factorisation 
  is numerically stable, computing and checking the conditioning of matrices.</p>
           
<p> Exceptions are thrown in the case  of numerical failure. They follow a
simple rule:<br>
    <em><strong>Bayes_filter_exception</strong> is thrown if an algorithm 
cannot   continue or guarantee the numerical post-conditions of a function</em></p>
           
<p>The only <em>exception guarantee</em> that Bayes++ makes when throwing 
  <strong><em>Bayes_filter_exception</em> </strong>from any function, is that
  no resources will be leaked. That is the <i>Weak guarantee</i> as defined
  by <a href="#Reference">Reference[9]</a>. Therefore, unless otherwise specified,
  the numeric value of a class's public state is <strong>undefined</strong> 
   after this exception is thrown by a member of a class.</p>
           
<p>Be aware that numerical post-conditions may be met even with extreme input 
  values. For example some inputs may result in overflow of a result. This 
  does not invalidate the post-conditions that the result is positive. Even 
  some <em>Not a Number</em> values may be valid. Therefore the results may 
  be computed without exception, but include <em>NaN</em> or <em>non-number</em> 
  values.</p>
           
<h3>Using exceptions in filter schemes</h3>
           
<p> What does this mean when using a filter Scheme? If the Scheme throws
an exception then, unless otherwise specified, the numeric value of its state 
 is undefined. Therefore you <strong>cannot</strong> use any function of the
 filter that has any preconditions on its numerical state. To continue  using
 a filter either the <em>init</em>() function should be used (which  has
no  pre-conditions) or the filter destructed.</p>
           
<p>Generally to access the public state of a Scheme you must first call the 
  <em>update</em>() function. If no exception is thrown then the post-conditions 
  of update are guaranteed. The post-conditions of <em>update</em>() may include
 such useful properties as the covariance matrix is PSD. Many application 
 specific tests may also be required. Just being PSD doesn't say much about 
  X. It could even have <em>Infinity</em> values on the diagonal! Conditioning, 
  trace, determinate, and variance bounds can all be applied at his point. 
  </p>
           
<h1>Rational</h1>
           
<h2>Polymorphic design</h2>
           
<p>Why are models and filter Schemes <strong>polymorphic</strong> ? OR why 
  are they implemented with virtual functions?</p>
           
<blockquote dir="ltr" style="margin-right: 0px;">                       
       
  <p>One alternative would be to use <strong>generic</strong> instead of
  <strong>polymorphic</strong> Schemes. This would implement Schemes as templates. 
  If the sizes of matrix   operations could be fixed generic models and schemes 
  would directly manipulate matrices for predefined size.</p>
      </blockquote>
           
<p>Bayes++ relies on a <strong>dual abstraction</strong>, separating numerical 
  schemes from model structure. To represent these as <strong>polymorphic
 hierarchies</strong> was a very important decision in the design of Bayes++.
 After a fair bit of use I still think it was the correct one. Why?</p>
           
<p>A) <em>Usage</em> : There are many (run time) polymorphic things I really 
  want to do with the library. I want to be able to build composite filtering 
  algorithms that switch models and schemes at run time. This requires BOTH 
  polymorphic filters and models AND runtime sizing of matrices.<br>
    A generic approach especially one parametised on matrix size would not
 allow this. The code size produced would also bloat massively. Not even
STL  parameterises its containers on size!</p>
           
<p>B) <em>Type safety</em>: There is surprising little orthoginality in filtering 
  schemes. The numerics often dictate restrictions or additional functionality. 
  The type hierarchy in Bayes++ provides a succinct and safe way to enforce 
  much of this structure.<br>
    In a Generic approach checking that a particular Scheme models a generic 
  concept correct would be very difficult to enforce.</p>
           
<p>C) <em>Compiler problems</em>: Generic programming in C++ has a nasty
syntax, is very slow to compile and pushes the limits of compiler reliability.</p>
           
<p>D) <em>Efficiency</em>: The run time overhead of a polymorphic filter
is negligible compared to the matrix algebra required. In fact using common 
  code may increase efficiency on a many computing architectures.</p>
           
<h1 style="text-align: left;" align="left">Matrix Representation</h1>
           
<p>Two modern C++ libraries can be used as the representation for matrices 
  (and   vectors) in Bayes++: <a href="http://www.boost.org">Boost uBlas</a> 
  and <a href="http://www.osl.iu.edu/research/mtl/" target="_top"> Matrix 
Template  Library</a> . Previous versions of the filters were implemented 
for <i>MPP</i>  and <i>TNT</i> but are no longer supported. Both are efficient 
and portable; their  designs include the matrices representations and linear 
algebra operations  required by Bayes++.</p>
           
<p> The Bayes++ implementation uses a set of adaptor classes in namespace 
  <i>Bayesian_filter_matrix</i> (FM for short) for matrix access. This system 
  should make it simpler to port the library to other matrix types. They support
 a common syntax with the following features.</p>
           
<ul>
      <li> Matrix row/column access with operator[]. Bayes++ implements many
  functions using row operations.</li>
       <li> Overloading of operator* or prod() for matrix multiplication. 
The  former may be removed to give better control of temporary creation and 
expression  complexity.</li>
       <li> For efficiency when implementing with MTL a few specific functions 
  such as add(a,b,c), rank_one_update etc</li>
         
</ul>
           
<p>For uBlas these function generally have no runtime overhead. This efficiency 
  is due to uBLAS's excellent expression template implementation. For  MTL 
 most functions are reasonably efficient but some  functions create matrix 
 temporaries to return values.</p>
           
<h2>Matrix library efficiency - Temporary object usage</h2>
           
<p> Most of the filters avoid using Matrix and Vec temporaries. They also 
  have optimisations for a constant (on construction) state size, noise and 
  observations. In particularly the UD_filter scheme avoids creating any temporary
 objects. All matrices maintain a fixed size, other then when the observation
 state size is varied.</p>
           
<p> Why are temporary matrix objects bad? The main difficulty is construction 
  and destruction of Matrices. This generally requires dynamic memory allocation 
   which is very slow. For small matrices this dominates compared to the cost
  of simple operations such as addition and multiplication.</p>
           
<p>There are three ways out of this problem.</p>
           
<blockquote>                               
  <p>1. Use fixed size matrices; they can (nearly) always be efficiently
allocated on   the stack.<br>
      2. Use stack based dynamic allocators (such as C99's dynamic arrays 
or  alloca) for temporaries.<br>
      3. Don't create temporaries. This is achievable with a combination
of  hard coding in the algorithms (pre-allocation) and by using a Matrix
library   with expression templates to avoid creating temporaries for return
values.</p>
   </blockquote>
           
<p> Bayes++ is implemented to avoid creating temporaries. At present my solution 
  is somewhat of a compromise. MTL is only moderately efficient on most C++ 
  compilers. On release code this results in a 50% performance reduction if
 temporaries are avoided. uBLAS attains close to optimal efficiency in many
 situations.</p>
           
<p>The UD_filter is a good illustration. It has been hand crafted to avoid 
  construction of any temporaries (I use it for embedding) and can be compiled 
  with either uBLAS, MTL or a special fast matrix library. UD_filter is heavily 
  optimised to use row operations to avoid indexing overheads. uBLAS and the
 special library achieve comparable results, which I believe are as fast
 as can be achieved.</p>
           
<p>On the flip side, the current Unscented_filter implementation shows some 
  of the problems. Because of my wrapper classes Row/Column of a 2D matrix 
  are not compatible with the Vec type. This results in a lot of unnecessary 
  copying into pre-allocated temporary vectors.</p>
           
<p>Future work includes a general method of avoiding implementation temporaries. 
  Filter schemes will be parameterised with helper objects that deal with
 all the temporaries required. These helper objects can then hide all the
 hard work of pre-allocation or dynamic allocation of temporary objects.</p>
           
<h1>Restrictions</h1>
           
<p>For numerical precision, all filter calculations use <b>doubles</b>. 
 Templates could be used o provide numerical type parameterisation but are 
  not. However the base class <i>Bayes_filter</i> defines <i>Float</i>, which
  is used throughout as the numerical type.</p>
           
<p>The <i>Bayes_filter</i> base class is constructed with constant state
size. This allows efficient use of the matrix library to implement the algorithms. 
 Each derived class requires additional representation and matrix temporaries 
 and where possible they are restricted to a constant size. In general, the 
 only unknown is the size of the observation state as parameterised  by the 
 observation model.</p>
           
<p>Where the state size varies efficient solutions are still possible using 
  either spares matrix representations or by recreating new filters with increased
 size.</p>
           
<h1>The future</h1>
           
<p>The Bayesian Filtering Classes are now a stable set of solution for the 
  majority   of linear, linearisable and non-linear filtering problems. I 
am  very happy with   their form and function. In particular, the two-layer 
abstraction  works extremely well. The classes show best practice in both 
their design  and in efficiency and numerical stability of the algorithms 
used. So where  to go from here?</p>
           
<p>The basic tenant of Bayesian theory is that the Likelihood function complete 
  captures all that is know to update the state. The Bayesian Filtering classes 
  now allow the modelling systems using Likelihood functions. At present the
 SIR filter is the only Likelihood filter. Sample filters will grow into
 a separate branch in the class hierarchy. A general set of adaptors will
be required to move between these varied representations.</p>
           
<p>To further improve the abstraction, the method of internal variable exposure 
  needs to be regularised. This will require the addition of a few classes 
  that hold and limit access to filters internal variables.</p>
           
<p>The ordering of parameters used in Scheme class constructors is  prone 
 to error. It requires the parameter list to be varied for each scheme used. 
 An extensible method of model parameterisation is required. A common parameterisation 
 could then be used with scheme constructor extracting the information they 
 require.</p>
           
<p>At present MTL does have a few drawbacks! In MTL assignment is by reference. 
  This means MTL objects do not have value semantics. I believe this is a
 fatal flaw in its design and causes many problems. I believe the design
 of uBLAS is more elegant and better constructed. Its expression template
 system works well. The future of Bayes++ probably is to use uBLAS as the
matrix library.</p>
           
<h2>STATE ABSTRACTION</h2>
           
<p> Can the state representation be abstracted away from the numerics of
a filter Scheme?</p>
           
<p>At the moment Bayes++ filter Schemes are a combination of the numerical 
  solution (e.g. innovation update) AND the representation (e.g. Covariance 
  matrix). Can these two functions be separated?</p>
         
<p>A highly sophisticated solution could use polymorphic (or generic) filter 
  algorithms that depend on the type (or traits) of the representation. I 
think this in unnecessarily complex. In Bayes++ a Scheme should only implement 
 one algorithm.</p>
           
<p>The problem I would like to address is a bit more limited. A lot of representations
  are naturally hierarchical and dynamic.<br>
    e.g. state := vehicle states &amp; feature states, feature states :=
{feature1   state &amp; feature2 state ....}</p>
           
<p>There would seem to be too possible ways to solve this</p>
         
<p>A) HARD: Use a state representation that represents this kind of hierarchy. 
  The filter schemes could therefore use hierarchical numerical solutions 
 that exploit the properties of the hierarchical state.</p>
           
<p>B) EASY: Allow a sparse matrix representation of state. If the sparse
representation is a Graph then the sort of augmented state representation
 in the example can be easily built without any copy overhead. Each scheme
 would then just use sparse matrix techniques in its numerical solution.
Mostly what is needed is Cholesky and QR decomposition and these have good
 sparse solutions. Obviously there would be a runtime overhead for this but
it would be great for Simultaneous Location and Mapping problems!</p>
           
<h1><a name="References"></a>References</h1>
           
<p>[1] "A New Approach for Filtering Nonlinear Systems", SJ Julier JK Uhlmann 
  HF Durrant-Whyte, American Control Conference 1995</p>
         
<p>[2] "Factorisation Methods for Discrete Sequential Estimation", Gerald 
  J. Bierman, ISBN 0-12-097350-2</p>
         
<p>[3] "Real time Kalman Filter Application", Mohinder S. Grewal, Angus P. 
  Andrews, ISBN 0-13-211335-X</p>
         
<p>[4] "Extension of Square-Root Filtering to Include Process Noise", P.
Dyer and S. McReynolds, Journal of Optimization Theory and Applications,
Vol.3 No.6 1969</p>
         
<p>[5] "Novel approach to nonlinear-non-Guassian Bayesian state estimation", 
  NJ Gordon, DJ Salmond, AFM Smith, IEE Proceeding-F, Vol.140 No.2 April 
1993</p>
         
<p>[6] "Tracking and Data Association", Y Bar-Shalom and T Fortmann, Academic 
  Press, ISBN 0-12-079760-7</p>
         
<p>[7] "Stochastic Models, Estimation, and Control", Peter S Maybeck, Academic 
  Press, ISBN 0-12-480701-1</p>
         
<p>[8] "Covariance Intersect"</p>
         
<p>[9] "Exception-Safety in Generic Components", David Abrahams, Proc. of 
  a   Dagstuhl Seminar 'Generic Programming', Lecture Notes on Computer Science 
  1766, <a href="http://www.boost.org/more/error_handling.html"
 target="_top"> http://www.boost.org/more/error_handling.html</a></p>
    <br>
     
</body>
</html>
