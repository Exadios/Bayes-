BayesFilter TODO List

TODO
	FIX Iterated filter: Require iterated model
	Information_root_filter does quite work for large Q values
	
	Replace UdU with as class using uBLAS row operations
	  This should UD_filter to use qa new recompose for its update
	Sharing state in filters: Provided constructors with state references
	observe_model structure changed
		Extened_filter observe_innovation parametrs is only Linear part of model as h() not used
	replace General_ with a form that works with functional forms

	Unscented_filter
		make XX public, requires specification of postcond on XX
		New forms Update can be done directly in factorised form
	UD_filter
		observeUD : return 0 for semi-definate (or zero)

	add a common base of h() in observe
	By products: add observe paramterisation for by products and temps

NEW FILTERS
	Factorised UKF or DDF2
	VSDF (factorised from) from Matt Deans
	SIR with Auxilary variables
	Sparse matrix implementations: Requires sparse UdU. Great care of ublas sparse_matrix as it is row/column major
	 This make a big difference to speed. Hard for prod(L,R) need to have oposite orienation

TO REVIEW
	Check consistency of model to filter
		Validate (z to h.Z) model sizes in predict and observe
	UDpredict catching Negative matrices in observe
	Check normalisation: Standard for is to normalise z. It is nice to have zp consistent with itself i.e. Hx
	Check when numerics fail for both state and obs semidefinate

ISSUES
	Enable Cov and Inf implmentations for PSD matricies where numerically possible. At present may be PD only

CODE RULES
	filter
		copy assigment, optimised to remove temps
	predict/observe
		in BayesFlt.h	must be virtual. The types are polymorphic
		in xxFlt.h		no vitual. These are specialised forms
	filter_matrix
		use Vec/Matrix x.assign (prod(a,b)); avoiding copy overhead

TESTING
	q_size > x_size
		Unscented_filter Don`t forget more noise injected for nonlinear model => different observe result!

DOCUMENTATION
	Describe generic schemes
	Why Gq noise model. It is always non signular. Important for any numerics
		Q may have zeros. But if we have G then linear algebra can still use inverse.
		This is also why inverse(G) is not needed in inverse model. It would not be defined for signular Q



// Look at using this implementation
template<class M>
bool lower_cholesky (M &m)
{
	using namespace ublas;
	bool negative = false;
	size_t size = m.size1();
	for (size_t i = 0; i < size; ++ i)
	{
		typename M::value_type t (m (i, i));
		// why this doesn't compile ?
		// matrix_vector_slice<M> mvs (m.column (i).project(range (i +1, size)));
		m.column (i).project(range (i +1, size)) -=
			prod(project (m, range (i + 1, size), range (0, i)),conj (project (row (m, i),range (0, i))));
		t -= inner_prod (conj (project (row(m, i), range (0,i))),project (row (m, i),range (0, i)));
		if (t <= 0)  {
			negative = true;
			break;
		};
		t = std::sqrt (t);
		m (i, i) = t;
		m.column (i).project(range (i +1, size)) /= t;
	}
	return negative;
}

UTriMatrix::value_type UCfactor (UTriMatrix& M, size_t n)
{
	assert (M.size1() == n);
	assert (M.size2() == n);
	bool negative = cholesky(M);
	// Estimate the reciprocal condition number
	if (negative)
		return -1;
	return UCrcond (M,n);

}

